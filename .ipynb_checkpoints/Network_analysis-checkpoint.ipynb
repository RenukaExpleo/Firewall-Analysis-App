{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea30588",
   "metadata": {},
   "source": [
    "# Routing network traffic based on firewall logs using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7e84f",
   "metadata": {},
   "source": [
    "## Problem Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce78821",
   "metadata": {},
   "source": [
    "The firewall system plays a vital role in network security by regulating network traffic through security rules. Setting up and maintaining the firewall system is a complex task, prone to human error and requiring regular modification to remain compliant with security policy. The proper routing of traffic ensures compliance with security policy and prevents unauthorized access to prohibited websites. Machine learning models can aid in the proper maintenance of firewall systems by analyzing historical data from firewall log reports, thus reducing the likelihood of errors and ensuring stable internet traffic.\n",
    "###### The aim of this case study is to explore the use of machine learning models for firewall maintenance and to demonstrate their effectiveness in identifying and blocking prohibited websites on a network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a159a7",
   "metadata": {},
   "source": [
    "# Data Understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76deea06",
   "metadata": {},
   "source": [
    "### Data Source: https://archive.ics.uci.edu/ml/datasets/Internet+Firewall+Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96748862",
   "metadata": {},
   "source": [
    "The dataset used in this case study is compiled by Fatih Ertam at Firat University, Turkey. The dataset contains 65532 instances with 12 features collected by logs of the university firewall system.\n",
    "\n",
    "The summary of the features is given below-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9c783",
   "metadata": {},
   "source": [
    "![data_desc](data_desc.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d0ec9",
   "metadata": {},
   "source": [
    "The action which is our response variable is categorized into four classes. Namely-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f5d0e",
   "metadata": {},
   "source": [
    "![Target_var_desc](Target.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386c76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f76c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries :\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pickle as pk\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bb414f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'log2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#To read csv File from locally stored file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'log2.csv'"
     ]
    }
   ],
   "source": [
    "#To read csv File from locally stored file\n",
    "data = pd.read_csv('log2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23faaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the dataset imported:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking unique values\n",
    "data['Action'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeeaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining the data frame for the shape, datatypes, NUlls etc\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabaa7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the numeric distribution of the data:\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing instances where Packets are beyond 99th percentile\n",
    "data = data[data['Bytes'] <= np.percentile(data['Bytes'], 99)]\n",
    "# removing instances where Packets are beyond 99th percentile\n",
    "data = data[data['Packets'] <= np.percentile(data['Packets'], 99)]\n",
    "# removing instances where Elapsed Time (sec) are beyond 99th percentile\n",
    "data = data[data['Elapsed Time (sec)'] <= np.percentile(data['Elapsed Time (sec)'], 99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot for countof each action\n",
    "sns.countplot(data=data, x = 'Action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80242dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Bytes\n",
    "data['Bytes'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(column=['Bytes'], by='Action', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0099e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Packets'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(column=['Packets'], by='Action', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping values from 'Action' column to numbers\n",
    "data['Action'] =  data[\"Action\"].map({'allow':1, 'deny':0, 'drop':2, 'reset-both':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(data.corr(),cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12d726",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1a90e",
   "metadata": {},
   "source": [
    "The dataset we are using has recorded port numbers on private devices as well as port numbers translated by NAT. Hence we can create two features for source and destination based on information if Port Translation (NAT) was required while passing the traffic. The features Source Port Translation and Destination Port Translation can be encoded by 1 if port numbers on devices and NAT are different indicating the requirement of NAT. Otherwise, they will be encoded by 0 indicating no translation was required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding translation features\n",
    "data['Source Port Translation'] = (data['Source Port'] != data['NAT Source Port']).astype('int')\n",
    "data['Destination Port Translation'] = (data['Destination Port'] != data['NAT Destination Port']).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3810d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building bidirectional graph by using port numbers on host devices\n",
    "HOST_NW = nx.DiGraph(name = \"Host\")\n",
    "HOST_NW.add_edges_from(data[['Source Port', 'Destination Port']].values)\n",
    "joblib.dump(HOST_NW, 'host_nw.pkl')\n",
    "# building bidirectional graph by using port numbers on NAT \n",
    "NAT_NW = nx.DiGraph(name = \"NAT\")\n",
    "NAT_NW.add_edges_from(data[['NAT Source Port', 'NAT Destination Port']].values)\n",
    "joblib.dump(NAT_NW, 'nat_nw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ports(nw, src, dst):\n",
    "    \"\"\"\n",
    "    Counts no. of common ports connected directly between src and dst ports.\n",
    "    Args:\n",
    "        nw: Network instance\n",
    "        src: Source Port\n",
    "        dst: Destination Port\n",
    "    Returns:\n",
    "        No. of common ports from intersection of set of neighbors of both src and dst ports\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(set(nw.neighbors(src)).intersection(set(nw.neighbors(dst))))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d210f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding features to the dataset\n",
    "data['Host CP'] = data.apply(lambda row: common_ports(HOST_NW, row['Source Port'], row['Destination Port']), axis = 1)\n",
    "data['NAT CP'] = data.apply(lambda row: common_ports(NAT_NW, row['NAT Source Port'], row['NAT Destination Port']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac25b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(nw, src, dst):\n",
    "    \"\"\"\n",
    "    Counts Jaccard index between src and dst ports.\n",
    "    Args:\n",
    "        nw: Network instance\n",
    "        src: Source Port\n",
    "        dst: Destination Port\n",
    "    Returns:\n",
    "        Jaccard Index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(set(nw.neighbors(src)).intersection(set(nw.neighbors(dst)))) / len(set(nw.neighbors(src)).union(set(nw.neighbors(dst))))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fa9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Host JI'] = data.apply(lambda row: common_ports(HOST_NW, row['Source Port'], row['Destination Port']), axis = 1)\n",
    "data['NAT JI'] = data.apply(lambda row: common_ports(NAT_NW, row['NAT Source Port'], row['NAT Destination Port']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salton_index(nw, src, dst):\n",
    "    \"\"\"\n",
    "    Counts Salton index between src and dst ports.\n",
    "    Args:\n",
    "        nw: Network instance\n",
    "        src: Source Port\n",
    "        dst: Destination Port\n",
    "    Returns:\n",
    "        Salton Index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(set(nw.neighbors(src)).intersection(set(nw.neighbors(dst)))) / np.sqrt(len(set(nw.neighbors(src))) * len(set(nw.neighbors(dst))))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Host SL'] = data.apply(lambda row: common_ports(HOST_NW, row['Source Port'], row['Destination Port']), axis = 1)\n",
    "data['NAT SL'] = data.apply(lambda row: common_ports(NAT_NW, row['NAT Source Port'], row['NAT Destination Port']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorensen_index(nw, src, dst):\n",
    "    \"\"\"\n",
    "    Counts Sorensen index between src and dst ports.\n",
    "    Args:\n",
    "        nw: Network instance\n",
    "        src: Source Port\n",
    "        dst: Destination Port\n",
    "    Returns:\n",
    "        Sorensen Index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(set(nw.neighbors(src)).intersection(set(nw.neighbors(dst)))) / (len(set(nw.neighbors(src))) + len(set(nw.neighbors(dst))))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc172fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Host SI'] = data.apply(lambda row: common_ports(HOST_NW, row['Source Port'], row['Destination Port']), axis = 1)\n",
    "data['NAT SI'] = data.apply(lambda row: common_ports(NAT_NW, row['NAT Source Port'], row['NAT Destination Port']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar_index(nw, src, dst):\n",
    "    \"\"\"\n",
    "    Counts Adamic-Adar index between src and dst ports.\n",
    "    Args:\n",
    "        nw: Network instance\n",
    "        src: Source Port\n",
    "        dst: Destination Port\n",
    "    Returns:\n",
    "        Adamic-Adar Index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ports = set(nw.neighbors(src)).intersection(set(nw.neighbors(dst)))\n",
    "        return 1/np.sum([np.log10(set(nw.neighbors(port))) for port in ports])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Host AA'] = data.apply(lambda row: common_ports(HOST_NW, row['Source Port'], row['Destination Port']), axis = 1)\n",
    "data['NAT AA'] = data.apply(lambda row: common_ports(NAT_NW, row['NAT Source Port'], row['NAT Destination Port']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_page_rank = nx.pagerank(HOST_NW)\n",
    "nat_page_rank = nx.pagerank(NAT_NW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad48f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Host Source PR'] = data.apply(lambda row: host_page_rank.get(row['Source Port'], 0), axis = 1)\n",
    "data['Host Destination PR'] = data.apply(lambda row: host_page_rank.get(row['Destination Port'], 0), axis = 1)\n",
    "data['NAT Source PR'] = data.apply(lambda row: nat_page_rank.get(row['NAT Source Port'], 0), axis = 1)\n",
    "data['NAT Destination PR'] = data.apply(lambda row: nat_page_rank.get(row['NAT Destination Port'], 0), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Action']\n",
    "X = data.drop(['Action'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6643010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size = 0.20, stratify = y, random_state = 859)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()\n",
    "robust_scaler_features = ['Bytes', 'Bytes Sent', 'Bytes Received', 'Packets', 'Elapsed Time (sec)', 'pkts_sent', 'pkts_received', 'Host Source PR', 'Host Destination PR', 'NAT Source PR', 'NAT Destination PR']\n",
    "X_train_robust_scaled = robust_scaler.fit_transform(X_train[robust_scaler_features])\n",
    "X_cv_robust_scaled = robust_scaler.transform(X_cv[robust_scaler_features])\n",
    "joblib.dump(robust_scaler, 'robust_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler_features = ['Host CP', 'NAT CP', 'Host JI', 'NAT JI', 'Host SL', 'NAT SL', 'Host SI', 'NAT SI', 'Host AA', 'NAT AA']\n",
    "X_train_std_scaled = std_scaler.fit_transform(X_train[std_scaler_features])\n",
    "X_cv_std_scaled = std_scaler.transform(X_cv[std_scaler_features])\n",
    "joblib.dump(std_scaler, 'std_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = np.hstack((X_train[X_train.columns[:4]], X_train_robust_scaled, X_train_std_scaled, X_train[['Source Port Translation', 'Destination Port Translation']]))\n",
    "X_cv_preprocessed = np.hstack((X_cv[X_cv.columns[:4]], X_cv_robust_scaled, X_cv_std_scaled, X_cv[['Source Port Translation', 'Destination Port Translation']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda2cab1",
   "metadata": {},
   "source": [
    "# BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89635a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(feature_matrix, train_labels):\n",
    "    \"\"\"\n",
    "    Generates random probabilities of output class proportional to their weights, where weights are no.of instances of that class divided by total no. \n",
    "    of instances in the train data\n",
    "    Args:\n",
    "        feature_matrix: Feature matrix of the dataset\n",
    "        train_labels: Actual labels of the instances in the train data\n",
    "    Returns:\n",
    "        random class labels as predictions\n",
    "    \"\"\"\n",
    "    classes = list(train_labels.unique())\n",
    "    # classes = no.of unique classes\n",
    "    class_weights = [np.sum(train_labels == i)/train_labels.shape[0] for i in train_labels.unique()]\n",
    "    # class_weights = list of class weights computed from train data\n",
    "    labels = np.random.choice(classes, size = feature_matrix.shape[0], p = class_weights)\n",
    "    # labels = predicted labels by random sampling of class labels proportional to their weights in train data\n",
    "    probs = np.zeros((labels.shape[0], len(classes)))\n",
    "    # probs = matrix of predicted probabilities\n",
    "    for i in range(labels.shape[0]):\n",
    "        # generate probabilities by adding random numbers between 0 and 1 to predicted class labels which ensures it has highest probability among all classes\n",
    "        probs[i] = classes.index(labels[i]) + np.random.random_sample(len(classes))\n",
    "        # normalizing probabilities of all classes so that they add up to 1\n",
    "        probs[i] /= np.sum(probs[i])\n",
    "    # probs = normalized random probabilities of output classes\n",
    "    return labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73efbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Purples\", fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model_train_labels, random_model_train_probs = baseline_model(X_train, y_train)\n",
    "random_model_test_labels, random_model_test_probs = baseline_model(X_cv, y_train)\n",
    "\n",
    "random_model_train_loss = log_loss(y_train, random_model_train_probs, eps=1e-15)\n",
    "print(f\"Log loss on Train Data using Random Model: {random_model_train_loss}\")\n",
    "random_model_test_loss = log_loss(y_cv, random_model_test_probs, eps=1e-15)\n",
    "print(f\"Log loss on Test Data using Random Model: {random_model_test_loss}\")\n",
    "plot_confusion_matrix(y_cv, random_model_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f3049",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11e463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d569b43a",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 2000, max_depth = 20, criterion = 'entropy', class_weight = 'balanced', n_jobs = -1, random_state = 859)\n",
    "rf.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_rf = CalibratedClassifierCV(rf, method = 'isotonic', cv = 'prefit')\n",
    "calibrator_rf.fit(X_cv_preprocessed, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_probs_rf = calibrator_rf.predict_proba(X_train_preprocessed)\n",
    "y_cv_probs_rf = calibrator_rf.predict_proba(X_cv_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_max_rf = np.amax(y_train_probs_rf, axis=1) #np.amax(y_train_probs, axis=1)\n",
    "y_cv_max_rf = np.amax(y_cv_probs_rf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a09657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_rf = log_loss(y_train, y_train_max_rf)\n",
    "cv_loss_rf = log_loss(y_cv, y_cv_max_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fdb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = calibrator_rf.predict(X_cv_preprocessed)\n",
    "accuracy_score(y_cv, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a14b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log loss on Train Data using Support Vectors Model: {train_loss_rf}\")\n",
    "print(f\"Log loss on Validation Data using Support Vectors Model: {cv_loss_rf}\")\n",
    "plot_confusion_matrix(y_cv, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(calibrator_rf, open(r'calibrator_rf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4c506",
   "metadata": {},
   "source": [
    "# LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_estimators = 750, max_depth = 4, objective = 'multiclass', class_weight = 'balanced', n_jobs = -1, random_state = 859)\n",
    "lgbm.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_lgbm = CalibratedClassifierCV(lgbm, method = 'isotonic', cv = 'prefit')\n",
    "calibrator_lgbm.fit(X_cv_preprocessed, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_probs_lgbm = calibrator_lgbm.predict_proba(X_train_preprocessed)\n",
    "y_cv_probs_lgbm = calibrator_lgbm.predict_proba(X_cv_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91871cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_max_lgbm = np.amax(y_train_probs_lgbm, axis=1) #np.amax(y_train_probs, axis=1)\n",
    "y_cv_max_lgbm = np.amax(y_cv_probs_lgbm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2430c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_lgbm = log_loss(y_train, y_train_max_lgbm)\n",
    "cv_loss_lgbm = log_loss(y_cv, y_cv_max_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94329990",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgbm = calibrator_lgbm.predict(X_cv_preprocessed)\n",
    "accuracy_score(y_cv, y_pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log loss on Train Data using Support Vectors Model: {train_loss_lgbm}\")\n",
    "print(f\"Log loss on Validation Data using Support Vectors Model: {cv_loss_lgbm}\")\n",
    "plot_confusion_matrix(y_cv, y_pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Source Port', 'Destination Port', 'NAT Source Port', 'NAT Destination Port',\n",
    "            'Bytes', 'Bytes Sent', 'Bytes Received', 'Packets', 'Elapsed Time (sec)', 'pkts_sent',\n",
    "            'pkts_received', 'Host Source PR', 'Host Destination PR', 'NAT Source PR', 'NAT Destination PR',\n",
    "            'Host CP', 'NAT CP', 'Host JI', 'NAT JI', 'Host SL', 'NAT SL', 'Host SI', 'NAT SI', 'Host AA',\n",
    "            'NAT AA', 'Source Port Translation', 'Destination Port Translation']\n",
    "# array of feature importances\n",
    "importances = rf.feature_importances_\n",
    "# sorting indices of importances in decreasing order\n",
    "indices = (np.argsort(importances))\n",
    "# plotting the horizontal barplot\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title('Feature Importances in Light GBM Classifier')\n",
    "plt.barh(range(len(indices)), importances[indices], color='green', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41493e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(calibrator_lgbm, 'calibrator_lgbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11c799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e45b08c",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901debae",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_svc = CalibratedClassifierCV(svc, method = 'isotonic', cv = 'prefit')\n",
    "calibrator_svc.fit(X_cv_preprocessed, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a33296",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_probs_svc = calibrator_svc.predict_proba(X_train_preprocessed)\n",
    "y_cv_probs_svc = calibrator_svc.predict_proba(X_cv_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eaa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_max_svc = np.amax(y_train_probs_svc, axis=1) #np.amax(y_train_probs, axis=1)\n",
    "y_cv_max_svc = np.amax(y_cv_probs_svc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81313b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_svc = log_loss(y_train, y_train_max_svc)\n",
    "cv_loss_svc = log_loss(y_cv, y_cv_max_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc = calibrator_svc.predict(X_cv_preprocessed)\n",
    "accuracy_score(y_cv, y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log loss on Train Data using Support Vectors Model: {train_loss_svc}\")\n",
    "print(f\"Log loss on Validation Data using Support Vectors Model: {cv_loss_svc}\")\n",
    "plot_confusion_matrix(y_cv, y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1efe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(calibrator_svc, open(r'calibrator_svc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b3c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b544f3",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(C = 1, penalty = 'l2', class_weight = 'balanced', n_jobs = -1, random_state = 859)\n",
    "logistic.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_logistic = CalibratedClassifierCV(logistic, method = 'isotonic', cv = 'prefit')\n",
    "calibrator_logistic.fit(X_cv_preprocessed, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82560b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_probs_log = calibrator_logistic.predict_proba(X_train_preprocessed)\n",
    "y_cv_probs_log = calibrator_logistic.predict_proba(X_cv_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ca15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_max_log = np.amax(y_train_probs_log, axis=1) #np.amax(y_train_probs, axis=1)\n",
    "y_cv_max_log = np.amax(y_cv_probs_log, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30966981",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_log = log_loss(y_train, y_train_max_log)\n",
    "cv_loss_log = log_loss(y_cv, y_cv_max_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log = calibrator_logistic.predict(X_cv_preprocessed)\n",
    "accuracy_score(y_cv, y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log loss on Train Data using Logistic Regression Model: {train_loss_log}\")\n",
    "print(f\"Log loss on Validation Data using Logistic Regression Model: {cv_loss_log}\")\n",
    "plot_confusion_matrix(y_cv, y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bbd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e8d600",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 8, weights = 'distance')\n",
    "knn.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_knn = CalibratedClassifierCV(knn, method = 'isotonic', cv = 'prefit')\n",
    "calibrator_knn.fit(X_cv_preprocessed, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2453144",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_probs_knn = calibrator_knn.predict_proba(X_train_preprocessed)\n",
    "y_cv_probs_knn = calibrator_knn.predict_proba(X_cv_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_max_knn = np.amax(y_train_probs_knn, axis=1) #np.amax(y_train_probs, axis=1)\n",
    "y_cv_max_knn = np.amax(y_cv_probs_knn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_knn = log_loss(y_train, y_train_max_knn)\n",
    "cv_loss_knn = log_loss(y_cv, y_cv_max_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6404371",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn = calibrator_knn.predict(X_cv_preprocessed)\n",
    "accuracy_score(y_cv, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"knn loss on Train Data using knnistic Regression Model: {train_loss_knn}\")\n",
    "print(f\"knn loss on Validation Data using knnistic Regression Model: {cv_loss_knn}\")\n",
    "plot_confusion_matrix(y_cv, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157bbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6ed80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dd56358",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32415e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def check(model, vector):\n",
    "    \"\"\"\n",
    "    Checks the run-time complexity of the model for a single log data. Gives mean of 100 test runs for reliability\n",
    "    Args:\n",
    "        model: Instance of the ML model\n",
    "        vector: vector of log data\n",
    "    Returns:\n",
    "        Mean run time of prediction\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    for i in range(100):\n",
    "        t = time.time()\n",
    "        pred = model.predict(vector)\n",
    "        times.append(time.time() - t)\n",
    "    return np.round(np.mean(times), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_times= {}\n",
    "Xc = X_cv.iloc[0].to_numpy()\n",
    "elapsed_times['KNeighbors Classifier'] = check(calibrator_knn, Xc.reshape(1, -1))\n",
    "elapsed_times['Logistic Regression'] = check(calibrator_logistic, Xc.reshape(1, -1))\n",
    "elapsed_times['Support Vectors'] = check(calibrator_svc, Xc.reshape(1, -1))\n",
    "elapsed_times['Random Forest Classifier'] = check(calibrator_rf, Xc.reshape(1, -1))\n",
    "elapsed_times['Light GBM Classifier'] = check(calibrator_lgbm, Xc.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229aa3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32569820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50817952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
